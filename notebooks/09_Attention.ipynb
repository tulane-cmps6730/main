{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Tulane](https://github.com/tulane-cmps6730/main/blob/main/img/banner.png?raw=true)\n",
        "\n",
        "<center>\n",
        "\n",
        "<font size=\"+3\">Attention</font>\n",
        "\n",
        "[Aron Culotta](https://cs.tulane.edu/~aculotta/)  \n",
        "[Tulane University](https://cs.tulane.edu/)\n",
        "\n",
        "<a href=\"http://colab.research.google.com/github/tulane-cmps6730/main/blob/main/notebooks/09_Attention.ipynb\">\n",
        "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"  width=10%/></a>\n",
        "<a href=\"https://github.com/tulane-cmps6730/main/tree/main\">\n",
        "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/GitHub_Invertocat_Logo.svg/240px-GitHub_Invertocat_Logo.svg.png\" width=6%/></a>\n",
        "\n",
        "In this module, we'll learn about attention mechanisms to better capture long-distance relationships in language sequences.\n",
        "\n",
        "</center>"
      ],
      "metadata": {
        "id": "u7t32OQMvRZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Motivation"
      ],
      "metadata": {
        "id": "f8OiXeEVwfGw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04LxYCDQvHc8"
      },
      "source": [
        "### LSTMs as language models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> While the acting was pretty good, overall the movie was quite bad.\n",
        "\n",
        "![figs/lstmclf1.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/lstmclf1.png?raw=1)\n",
        "\n",
        "\n",
        "Each hidden state is now a **context-dependent** word representation.\n",
        "\n",
        "<br><br><br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "_wYMuHKrw6iX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bidirectional LSTMs"
      ],
      "metadata": {
        "id": "jO4DP-kjwwFG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's no reason to limit ourselves to left-to-right ordering:\n",
        "\n",
        "![figs/lstmclf2.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/lstmclf2.png?raw=1)\n",
        "\n",
        "Word representation is now the concatenation of hidden vectors from both directions.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ctDXLahQwssF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTMs for Classification"
      ],
      "metadata": {
        "id": "14jFC71cwyqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![figs/lstmclf3.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/lstmclf3.png?raw=1)\n",
        "\n",
        "What are our options here?\n",
        "\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "**concatenate**: $h_1 \\oplus h_2 \\oplus \\ldots \\oplus h_n$\n",
        "\n",
        "What's wrong with this?\n",
        "\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "average: $\\frac{1}{n}\\sum_i^n h_i$\n",
        "\n",
        "What's wrong with this?\n",
        "\n",
        "<br><br><br><br>\n",
        "\n",
        "$h_{\\hbox{bad}}$ contributes as much to the average as $h_{\\hbox{the}}$!\n",
        "\n",
        "<br><br>\n",
        "\n",
        "How can we get something like a weighted average?\n",
        "\n",
        "$\\frac{\\sum_i^n w_i h_i}{\\sum_i^n w_i}$\n",
        "\n",
        "<br><br><br>\n",
        "We need a way to\n",
        "\n",
        "- get a fixed representation from an arbitrary list of values\n",
        "- have the representation weighted by the importance of each input value"
      ],
      "metadata": {
        "id": "Lh6cwOZJwuhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Attention\n"
      ],
      "metadata": {
        "id": "kapVP3M5xCIZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEb4XthLvHc8"
      },
      "source": [
        "\n",
        "> Given a set of vector **values**, and a vector **query**,  \n",
        "> **attention** is a technique to compute a weighted sum of the values, dependent on the query.\n",
        "\n",
        "- a selective summary of the values based on the query\n",
        "- gives a fixed-size representation of the values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall approach\n"
      ],
      "metadata": {
        "id": "bnX9QdcvxEa5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkgFJPUvHc9"
      },
      "source": [
        "\n",
        "**Input**: sequence of value vectors $\\mathbf{h}_1 \\ldots \\mathbf{h}_n \\in \\mathbb{R}^{d_h}$ and a query vector $\\mathbf{q} \\in \\mathbb{R}^{d_q}$\n",
        "\n",
        "1. Compute **attention scores** $\\mathbf{s} \\in \\mathbb{R}^n$\n",
        "  - we'll see how in a moment\n",
        "\n",
        "\n",
        "2. Apply softmax to get the **attention distribution** $\\alpha$:\n",
        "  - $\\alpha = \\mathrm{softmax}(\\mathbf{s}) \\in \\mathbb{R}^n$\n",
        "  \n",
        "  \n",
        "3. Compute the **attention output**, the sum of values weighted by attention distribution:\n",
        "  - $\\mathbf{a} = \\sum_i^n \\alpha_i \\mathbf{h}_i \\in \\mathbb{R}^{d_h}$\n",
        "  - $\\mathbf{a}$ then becomes input features for classification layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of attention\n"
      ],
      "metadata": {
        "id": "ixyhlyh1xISE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGVdkp1LvHc9"
      },
      "source": [
        "\n",
        "\n",
        "The attention scores should somehow combine the query vector with the value vectors to score the importance of each value. Common choices include:\n",
        "\n",
        "- **Dot product:** $~~~\\mathbf{s}_i = \\mathbf{q} \\cdot \\mathbf{h}_i \\in \\mathbb{R}$\n",
        "  - this assumes $d_h == d_q$\n",
        "\n",
        "\n",
        "- **Multiplicative attention:**   $~~~\\mathbf{s}_i = \\mathbf{q}^T \\mathbf{W} \\mathbf{h}_i \\in \\mathbb{R}$\n",
        "  - must learn the matrix $W \\in \\mathbb{R}^{d_q \\times d_h}$\n",
        "  - c.f., [`torch.nn.Bilinear`](https://pytorch.org/docs/stable/generated/torch.nn.Bilinear.html)\n",
        "  \n",
        "\n",
        "- **Additive attention:**   $~~~\\mathbf{s}_i = \\mathbf{v}^T \\mathrm{tanh}(\\mathbf{W_1} \\mathbf{h}_i  + \\mathbf{W_2}\\mathbf{q}) \\in \\mathbb{R}$\n",
        "  - must learn vector $\\mathbf{v} \\in \\mathbb{R}^{d_v}$ and the matrices $W_1 \\in \\mathbb{R}^{d_v \\times d_h}$, $W_2 \\in \\mathbb{R}^{d_v \\times d_q}$,=\n",
        "  - must select **attention dimensionality** $d_v$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Learning the query vector\n"
      ],
      "metadata": {
        "id": "-CO_vzVSxM5T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U5eWZbfvHc9"
      },
      "source": [
        "\n",
        "\n",
        "But wait, we don't know what $\\mathbf{q}$ should be!\n",
        "\n",
        "We'll have to learn it.\n",
        "\n",
        "$\\mathbf{s_i} = \\mathrm{tanh}(W_s \\cdot \\mathbf{h}_i) \\in \\mathbb{R}~~~~~~~~~ W_s \\in \\mathbb{R^{d_h}}$\n",
        "\n",
        "$\\alpha = \\mathrm{softmax}({\\mathbf{s}})$\n",
        "\n",
        "$\\mathbf{a} = \\sum_i^n \\alpha_i \\mathbf{h}_i \\in \\mathbb{R}^{d_h}$\n",
        "\n",
        "$y = \\mathrm{softmax}(W_y \\cdot \\mathbf{a})$\n",
        "<br><br>\n",
        "\n",
        "\n",
        "![figs/lstmclf4.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/lstmclf4.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Attention for Language Models"
      ],
      "metadata": {
        "id": "ioD31_UvxUbm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnX9hjiXvHc9"
      },
      "source": [
        "\n",
        "\n",
        "- LSTMs go through a lot of trouble to enable long-range dependencies\n",
        "\n",
        "- What if we just let every word in a sentence influence every other word ?!\n",
        "\n",
        "\n",
        "![figs/selfatt1.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/selfatt1.png?raw=1)\n",
        "\n",
        "We can use attention to determine which other words are important to predict the next word $t$.\n",
        "\n",
        "$$h_t =  \\sum_{i\\ne t}^n \\alpha_i \\mathbf{h}_i$$\n",
        "\n",
        "\n",
        "Is this scalable?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "\n",
        "By removing sequential dependencies, we can compute embeddings in parallel for each sentence.\n",
        "- In complexity speak, we've increased the Work but decreased the Span, so it is more parallelizable\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention-based word representations"
      ],
      "metadata": {
        "id": "nF36atMHxfZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "By applying attention to each token in the input, we compute hidden representations $h_i$ for each input word.\n",
        "- Each $h_i$ depends on all the other words in the input.\n",
        "\n",
        "\n",
        "$$\\mathbf{v}_j = V\\mathbf{x_j} ~~~V\\in \\mathbb{R}^{dxd} ~~~$$ **values for node j**\n",
        "\n",
        "$$\\mathbf{k}_j = K \\mathbf{x}_j ~~~K\\in \\mathbb{R}^{dxd} ~~~$$ **keys for node j**\n",
        "\n",
        "$$\\mathbf{q}_i = Q\\mathbf{x_i}~~~$$ **query for node i**\n",
        "\n",
        "$$\\alpha_{ij} = \\frac{\\exp(\\mathbf{q}_i \\cdot \\mathbf{k}_j)}{\\sum_{j'} \\exp({\\mathbf{q}_i \\cdot \\mathbf{k}_{j'})}} ~~~$$ **affinities between node i and j**\n",
        "\n",
        "$$\\mathbf{h}_i = \\sum_{j=1}^n \\alpha_{ij} \\mathbf{v}_j$$\n",
        "\n",
        "\n",
        "$\\mathbf{h}_i$ is the contextual representation of input $\\mathbf{x}_i$.  $\\alpha_{ij}$ controls the strength of each contribution from $v_j$.\n",
        "\n",
        "<br>\n",
        "\n",
        "This tells us **what information from what other tokens, should be used in representing** $\\mathbf{x}_i$.\n",
        "\n",
        "\n",
        "The matrices $K$, $Q$, $V$ allow us to use different views of each $\\mathbf{x}_i$ for the different roles of key, query, and value.\n",
        "\n",
        "<br>\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/selfattention.png?raw=1\" width=60%/>\n",
        "\n",
        "[source](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)\n",
        "\n"
      ],
      "metadata": {
        "id": "PqtqWcopxcLM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Position"
      ],
      "metadata": {
        "id": "zygxcHKayDIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "But wait, the above approach completely ignores word order!!\n",
        "\n",
        "<br><br><br>\n",
        "\n",
        "**Idea**: explicitly add the position of a word in its representation.\n",
        "\n",
        "- E.g., we can represent a binary string $p$ encoding the position of a word; e.g. $000, 001, 010, 011,... 111$ for sequences up to length 8.\n",
        "\n",
        "\n",
        "- Then, we can represent each input embedding $h_i$ as the concatenation $h_i \\oplus p_i$ or alternatively as the sum $h_i + p_i$.\n",
        "\n",
        "\n",
        "- We can also get more fancy and learn an embedding for $p_i$.\n",
        "  - requires picking a maximum sentence length\n",
        "  - most common approach\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8couM3aMyBFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Simple attention example"
      ],
      "metadata": {
        "id": "5S24YoMLzJXl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tXyJIH8vHc-"
      },
      "source": [
        "\n",
        "\n",
        "Let's walk through a simple example of an attention model for binary classification.\n",
        "\n",
        "We'll assume there are three words in the world: $a,b,c$. We will assume that $a$ and $b$ matter for the classification, but $c$ doesn't.\n",
        "\n",
        "We want to see if the attention weights will \"pay attention to\" $a$ and $b$ and not $c$.\n",
        "\n",
        "\n",
        "Below is a simple implementation of attention where we assume:\n",
        "- There is no embedding layer; so, we just use the `x` input vectors directly instead of `h`.\n",
        "- We are doing binary classification, so we use Sigmoid instead of Softmax.\n",
        "\n",
        "This would represent a problem where we want to classify a document into one of two classes, using the raw one-hot encoding of each word as input.\n",
        "\n",
        "$\\mathbf{s_i} = \\mathrm{tanh}(W_s \\cdot \\mathbf{x}_i) \\in \\mathbb{R}~~~~~~~~~ W_s \\in \\mathbb{R^{d_x}}$\n",
        "\n",
        "$\\alpha = \\mathrm{softmax}({\\mathbf{s}})$\n",
        "\n",
        "$\\mathbf{a} = \\sum_i^n \\alpha_i \\mathbf{x}_i \\in \\mathbb{R}^{d_x}$\n",
        "\n",
        "$ y = \\mathrm{sigmoid}(W_y \\cdot \\mathbf{a}) $\n",
        "\n",
        "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/lstmclf4.png?raw=1\" width=\"40%\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qs5taqB7vHc-"
      },
      "outputs": [],
      "source": [
        "# Let's package these computations up inside of an nn.Module so we can learn W_s and W_y.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "    def __init__(self, input_size, verbose=False):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        # Since we don't know what the output size will be (the number of tokens), we can't use nn.Linear:\n",
        "        # self.input_to_attention = nn.Linear(input_size, output_size=??, bias=False)\n",
        "        # Instead, we'll just use the Parameter object, which is a trainable tensor.\n",
        "        self.W_s = nn.Parameter(torch.randn(input_size, dtype=torch.float64))\n",
        "\n",
        "        # W_y: hidden to prediction\n",
        "        self.W_y = nn.Linear(input_size, 1, bias=False, dtype=torch.float64)\n",
        "\n",
        "        self.verbose = verbose\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=0)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # \"@\" is the matrix multiply operator\n",
        "        # This computes s = tanh(x * W_s) for all tokens at the same time.\n",
        "        s = self.tanh(x @ self.W_s)\n",
        "        # normalize using softmax\n",
        "        alpha = self.softmax(s)\n",
        "        # compute final combined embeddings\n",
        "        a = alpha @ x\n",
        "        # classify\n",
        "        y = self.sigmoid(self.W_y(a))\n",
        "        if self.verbose:\n",
        "            print('s=\\n', s)\n",
        "            print('alpha=\\n', alpha)\n",
        "            print('a=\\n', a)\n",
        "            print('y=\\n', y)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ8ONzyFvHc_"
      },
      "source": [
        "Let's make some fake data where the input vocabulary `['a', 'b', 'c']`. Generate data such that:\n",
        "- Documents with `a` are likely to be positive.\n",
        "- Documents with `b` are likely to be negative.\n",
        "- Documents with `c` are equally likely to be positive or negative.\n",
        "\n",
        "So, we would expect our attention layer to pay attention to words `a` and `b` but ignore `c`.\n",
        "\n",
        "And, we expect the classification layer to have large positive weights for `a` and large negative weights for `b`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q2a5yEAWvHc_",
        "outputId": "0f054a59-3ce0-4ecc-89fd-205ce8ab58fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [0., 0., 1.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 0., 1.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 1-hot encodings of the \"words\" a,b,c\n",
        "a = [1,0,0]\n",
        "b = [0,1,0]\n",
        "c = [0,0,1]\n",
        "\n",
        "# e.g., the document containing a, c, a, c\n",
        "torch.tensor([a,c,a,c], dtype=torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "weCXEj9QvHc_",
        "outputId": "93cc214e-d874-40e3-ebc8-10a2859a8224",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s=\n",
            " tensor([0.9158, 0.3462, 0.9158, 0.3462], dtype=torch.float64,\n",
            "       grad_fn=<TanhBackward0>)\n",
            "alpha=\n",
            " tensor([0.3193, 0.1807, 0.3193, 0.1807], dtype=torch.float64,\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "a=\n",
            " tensor([0.6387, 0.0000, 0.3613], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward4>)\n",
            "y=\n",
            " tensor([0.4922], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.4922], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "model = SimpleAttention(input_size=3, verbose=True)\n",
        "model(torch.tensor([a,c,a,c], dtype=torch.float64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eVoJJZs_vHc_",
        "outputId": "c3708f17-c60f-4ceb-cbe3-16d294600732",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[1., 0., 0.],\n",
            "         [0., 0., 1.],\n",
            "         [1., 0., 0.],\n",
            "         [0., 0., 1.]]], dtype=torch.float64)\n",
            "tensor(1., dtype=torch.float64)\n"
          ]
        }
      ],
      "source": [
        "# create training instances where\n",
        "# a -> label=1\n",
        "# b -> label=0\n",
        "# c -> label=random\n",
        "X = []\n",
        "y = []\n",
        "for _ in range(10):\n",
        "    X.append(torch.tensor([a,c,a,c], dtype=torch.float64))\n",
        "    y.append(1)\n",
        "\n",
        "for _ in range(10):\n",
        "    X.append(torch.tensor([b,c,b,c], dtype=torch.float64))\n",
        "    y.append(0)\n",
        "X = torch.stack(X)\n",
        "# print first training instance\n",
        "print(X[:1])\n",
        "y = torch.tensor(y, dtype=torch.float64)\n",
        "# print first label\n",
        "print(y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4GTwVXykvHc_"
      },
      "outputs": [],
      "source": [
        "data = list(zip(X,y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UGrPWF2CvHc_",
        "outputId": "0ac49b85-f9de-497f-bc05-d0340a6aec00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 78.32it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleAttention(\n",
              "  (W_y): Linear(in_features=3, out_features=1, bias=False)\n",
              "  (tanh): Tanh()\n",
              "  (softmax): Softmax(dim=0)\n",
              "  (sigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, data, epochs=20, learning_rate=0.4):\n",
        "    torch.random.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate)\n",
        "    model.verbose = False\n",
        "    loss_val = []\n",
        "    # main training loop\n",
        "    for epoch in tqdm(range(epochs), total=epochs):\n",
        "        np.random.shuffle(data)\n",
        "        optimizer.zero_grad() # reset all the gradient information\n",
        "        for X, y in data:\n",
        "            result = model.forward(X)\n",
        "            loss = criterion(result[0], y)\n",
        "            loss.backward()      # computes all the gradients\n",
        "        optimizer.step()     # update parameters\n",
        "        loss_val.append(loss.item())\n",
        "    plt.figure()\n",
        "    plt.plot(loss_val, 'bo-')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('instances')\n",
        "    plt.show()\n",
        "    return model\n",
        "\n",
        "\n",
        "att = SimpleAttention(input_size=3)\n",
        "train_model(att, data, learning_rate=.4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3h54hFFuvHdA",
        "outputId": "23635d58-768f-4093-b8d7-0ecb3977dc82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input=\n",
            " tensor([[1., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [1., 0., 0.],\n",
            "        [0., 0., 1.]], dtype=torch.float64)\n",
            "s=\n",
            " tensor([-0.9996, -0.9994, -0.9996, -0.9994], dtype=torch.float64,\n",
            "       grad_fn=<TanhBackward0>)\n",
            "alpha=\n",
            " tensor([0.2500, 0.2500, 0.2500, 0.2500], dtype=torch.float64,\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "a=\n",
            " tensor([0.4999, 0.0000, 0.5001], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward4>)\n",
            "y=\n",
            " tensor([0.9962], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9962], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "att.verbose=True\n",
        "# input is a,c,a,c\n",
        "print('input=\\n', X[0])\n",
        "att.forward(X[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XbDtj-ZFvHdA",
        "outputId": "f22e3a14-e793-43fa-e7c1-78a92ae681cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([-4.2902,  3.8674, -4.0363], dtype=torch.float64, requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "att.W_s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "X5yneEkkvHdA",
        "outputId": "1728ace3-4482-4592-c870-b1834563c412",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[ 5.1541, -6.3198,  5.9728]], dtype=torch.float64, requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "att.W_y.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SH2K0nUovHdA",
        "outputId": "7e2b00ea-ba20-40d7-887b-bc36b280327b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s=\n",
            " tensor([-0.9996, -0.9994, -0.9996, -0.9994, -0.9996, -0.9994, -0.9996, -0.9994,\n",
            "        -0.9996, -0.9994], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
            "alpha=\n",
            " tensor([0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
            "        0.1000], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "a=\n",
            " tensor([0.4999, 0.0000, 0.5001], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward4>)\n",
            "y=\n",
            " tensor([0.9962], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9962], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "att.forward(torch.tensor([a,c,a,c,a,c,a,c,a,c], dtype=torch.float64))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aeLs3Qg2vHdA",
        "outputId": "fda9fb71-e6bd-4e74-ed34-8e7b1ac0db8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s=\n",
            " tensor([-0.9996,  0.9991], dtype=torch.float64, grad_fn=<TanhBackward0>)\n",
            "alpha=\n",
            " tensor([0.1193, 0.8807], dtype=torch.float64, grad_fn=<SoftmaxBackward0>)\n",
            "a=\n",
            " tensor([0.1193, 0.8807, 0.0000], dtype=torch.float64,\n",
            "       grad_fn=<SqueezeBackward4>)\n",
            "y=\n",
            " tensor([0.0070], dtype=torch.float64, grad_fn=<SigmoidBackward0>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0070], dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "att.forward(torch.tensor([a,b], dtype=torch.float64))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Group Task\n"
      ],
      "metadata": {
        "id": "ABbCoBSi1v4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modify the `SimpleAttention` implementation above to use sigmoid instead of softmax to compute the `alpha` values. How does this affect the `alpha` values in the subsequent example? Why do you think this is? Is this good or bad?"
      ],
      "metadata": {
        "id": "lBcU9GjN112E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1YwQFKzvHdB"
      },
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- https://web.stanford.edu/class/cs224n/"
      ],
      "metadata": {
        "id": "-MRdAc_O01gu"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f8OiXeEVwfGw",
        "jO4DP-kjwwFG",
        "kapVP3M5xCIZ",
        "-CO_vzVSxM5T",
        "ioD31_UvxUbm",
        "zygxcHKayDIH",
        "5S24YoMLzJXl",
        "ABbCoBSi1v4r",
        "V1YwQFKzvHdB"
      ],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}