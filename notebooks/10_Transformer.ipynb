{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7t32OQMvRZq"
   },
   "source": [
    "![Tulane](https://github.com/tulane-cmps6730/main/blob/main/img/banner.png?raw=true)\n",
    "\n",
    "<center>\n",
    "\n",
    "<font size=\"+3\">Transformers</font>\n",
    "\n",
    "[Aron Culotta](https://cs.tulane.edu/~aculotta/)  \n",
    "[Tulane University](https://cs.tulane.edu/)\n",
    "\n",
    "<a href=\"http://colab.research.google.com/github/tulane-cmps6730/main/blob/main/notebooks/10_Transformers.ipynb\">\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"  width=10%/></a>\n",
    "<a href=\"https://github.com/tulane-cmps6730/main/tree/main\">\n",
    "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/GitHub_Invertocat_Logo.svg/240px-GitHub_Invertocat_Logo.svg.png\" width=6%/></a>\n",
    "\n",
    "In this module, we'll learn about the transformer mechanism, which uses attention to model language.\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8OiXeEVwfGw"
   },
   "source": [
    "<hr size=10 color=#285C4D>\n",
    "\n",
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GPT in ChatGPT stands for...\n",
    "\n",
    "- **G**enerative\n",
    "- **P**retrained\n",
    "- **T**ransformer\n",
    "\n",
    "Today we'll learn what this means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnX9QdcvxEa5"
   },
   "source": [
    "### Attention Review\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> Given a set of vector **values**, and a vector **query**,  \n",
    "> **attention** is a technique to compute a weighted sum of the values, dependent on the query.\n",
    "\n",
    "- a selective summary of the values based on the query\n",
    "- gives a fixed-size representation of the values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwkgFJPUvHc9"
   },
   "source": [
    "\n",
    "**Input**: sequence of value vectors $\\mathbf{h}_1 \\ldots \\mathbf{h}_n \\in \\mathbb{R}^{d_h}$ and a query vector $\\mathbf{q} \\in \\mathbb{R}^{d_q}$\n",
    "\n",
    "1. Compute **attention scores** $\\mathbf{s} \\in \\mathbb{R}^n$\n",
    "  - we'll see how in a moment\n",
    "\n",
    "\n",
    "2. Apply softmax to get the **attention distribution** $\\alpha$:\n",
    "  - $\\alpha = \\mathrm{softmax}(\\mathbf{s}) \\in \\mathbb{R}^n$\n",
    "  \n",
    "  \n",
    "3. Compute the **attention output**, the sum of values weighted by attention distribution:\n",
    "  - $\\mathbf{a} = \\sum_i^n \\alpha_i \\mathbf{h}_i \\in \\mathbb{R}^{d_h}$\n",
    "  - $\\mathbf{a}$ then becomes input features for classification layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioD31_UvxUbm"
   },
   "source": [
    "<hr size=10 color=#285C4D>\n",
    "\n",
    "## Self-Attention for Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnX9hjiXvHc9"
   },
   "source": [
    "\n",
    "- We can use the idea of attention to predict the next word in a sentence.\n",
    "- Attention lets us model long-range dependencies\n",
    "- The prediction for word $i$ depends on **all previous words**\n",
    "\n",
    "\n",
    "\n",
    "![figs/m10selfatt.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10selfatt.png?raw=1)\n",
    "\n",
    "![figs/m10selfatt2.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10selfatt2.png?raw=1)\n",
    "\n",
    "We can use attention to determine which other words are important to predict the next word $t$.\n",
    "\n",
    "$$\\mathbf{a}_i =  \\sum_{j \\le i}^n \\alpha_{ij} \\mathbf{v}_j$$\n",
    "\n",
    "\n",
    "\n",
    "Each input embedding plays three different roles:\n",
    "\n",
    "- As the current focus of attention when being compared to all of the other preceding inputs. We’ll refer to this role as a **query**.\n",
    "- As a preceding input being compared to the current focus of attention. We’ll refer to this role as a **key**.\n",
    "- As a **value** used to compute the output for the current focus of attention.\n",
    "\n",
    "We use three different weight matrices to represent each role.\n",
    "\n",
    "$\\mathbf{v}_j = V\\mathbf{x_j} ~~~V\\in \\mathbb{R}^{dxd} ~~~$ **values for node j**\n",
    "\n",
    "$\\mathbf{k}_j = K \\mathbf{x}_j ~~~K\\in \\mathbb{R}^{dxd} ~~~$ **keys for node j**\n",
    "\n",
    "$\\mathbf{q}_i = Q\\mathbf{x_i}~~~$ **query for node i**\n",
    "\n",
    "$\\alpha_{ij} = \\frac{\\exp(\\mathbf{q}_i \\cdot \\mathbf{k}_j)}{\\sum_{j'} \\exp({\\mathbf{q}_i \\cdot \\mathbf{k}_{j'})}} ~~~$ **affinities between node i and j**\n",
    "\n",
    "**nb:** we often scale down dot product for better numerical stability:\n",
    "$\\alpha_{ij} = \\frac{\\exp(\\mathbf{q}_i \\cdot \\mathbf{k}_j) / \\sqrt{d_k}}{\\sum_{j'} \\exp({\\mathbf{q}_i \\cdot \\mathbf{k}_{j'})}}$\n",
    "\n",
    "**Putting it all together:**\n",
    "\n",
    "![figs/m10selfatt3.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10selfatt3.png?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings and Unembedding\n",
    "\n",
    "To use self-attention for language modeling, we need two additional layers.\n",
    "- Embedding layer: from one-hot encoding of input word to a dense word embedding vector\n",
    "- Unembedding layer: from a dense word embedding vector to a distribution over output words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's package these computations up inside of an nn.Module so we can learn W_s and W_y.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleSelfAttention(nn.Module):\n",
    "    def __init__(self, input_size, verbose=False):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        # Since we don't know what the output size will be (the number of tokens), we can't use nn.Linear:\n",
    "        # self.input_to_attention = nn.Linear(input_size, output_size=??, bias=False)\n",
    "        # Instead, we'll just use the Parameter object, which is a trainable tensor.\n",
    "        self.W_s = nn.Parameter(torch.randn(input_size, dtype=torch.float64))\n",
    "\n",
    "        # W_y: hidden to prediction\n",
    "        self.W_y = nn.Linear(input_size, 1, bias=False, dtype=torch.float64)\n",
    "\n",
    "        self.verbose = verbose\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"@\" is the matrix multiply operator\n",
    "        # This computes s = tanh(x * W_s) for all tokens at the same time.\n",
    "        s = self.tanh(x @ self.W_s)\n",
    "        # normalize using softmax\n",
    "        alpha = self.softmax(s)\n",
    "        # compute final combined embeddings\n",
    "        a = alpha @ x\n",
    "        # classify\n",
    "        y = self.sigmoid(self.W_y(a))\n",
    "        if self.verbose:\n",
    "            print('s=\\n', s)\n",
    "            print('alpha=\\n', alpha)\n",
    "            print('a=\\n', a)\n",
    "            print('y=\\n', y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nF36atMHxfZA"
   },
   "source": [
    "### Attention-based word representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqtqWcopxcLM"
   },
   "source": [
    "\n",
    "\n",
    "By applying attention to each token in the input, we compute hidden representations $h_i$ for each input work.\n",
    "- Each $h_i$ depends on all the other words in the input.\n",
    "\n",
    "\n",
    "$$\\mathbf{v}_j = V\\mathbf{x_j} ~~~V\\in \\mathbb{R}^{dxd} ~~~$$ **values for node j**\n",
    "\n",
    "$$\\mathbf{k}_j = K \\mathbf{x}_j ~~~K\\in \\mathbb{R}^{dxd} ~~~$$ **keys for node j**\n",
    "\n",
    "$$\\mathbf{q}_i = Q\\mathbf{x_i}~~~$$ **query for node i**\n",
    "\n",
    "$$\\alpha_{ij} = \\frac{\\exp(\\mathbf{q}_i \\cdot \\mathbf{k}_j)}{\\sum_{j'} \\exp({\\mathbf{q}_i \\cdot \\mathbf{k}_{j'})}} ~~~$$ **affinities between node i and j**\n",
    "\n",
    "$$\\mathbf{h}_i = \\sum_{j=1}^n \\alpha_{ij} \\mathbf{v}_j$$\n",
    "\n",
    "\n",
    "$\\mathbf{h}_i$ is the contextual representation of input $\\mathbf{x}_i$.  $\\alpha_{ij}$ controls the strength of each contribution from $v_j$.\n",
    "\n",
    "<br>\n",
    "\n",
    "This tells us **what information from what other tokens, should be used in representing** $\\mathbf{x}_i$.\n",
    "\n",
    "\n",
    "The matrices $K$, $Q$, $V$ allow us to use different views of each $\\mathbf{x}_i$ for the different roles of key, query, and value.\n",
    "\n",
    "<br>\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/selfattention.png?raw=1\" width=60%/>\n",
    "\n",
    "[source](https://web.stanford.edu/class/cs224n/readings/cs224n-self-attention-transformers-2023_draft.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtUIgldHyhIh"
   },
   "source": [
    "## Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b40_cBX2yfCy"
   },
   "source": [
    "\n",
    "A **transformer** block is a collection of ideas. But, at its core, it contains a *self-attention layer* and *positional embeddings*.\n",
    "\n",
    "Other tricks include:\n",
    "\n",
    "- **multi-head attention**: multiple attention layers are run for each input sentence\n",
    "  - e.g., one can pay attention to syntax, another semantics, etc.\n",
    "- **residual connections** between layers: $X_i = X_{i-1} + \\mathrm{Layer}(X_i)$\n",
    "- **layer normalization** compute *z*-scores for values in each layer (subtract mean and divide by standard deviation)\n",
    "  - can improve training convergence by placing weights in same \"scale\" across layers\n",
    "  - Normalization is done separately for each token:\n",
    "  - $$\\mu_i = \\frac{1}{d}\\sum_{j=1}^d \\mathbf{h}_{ij}$$\n",
    "\n",
    "<br><br>\n",
    "\n",
    "![figs/transformer.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/transformer.png?raw=1)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Translation\n",
    "\n",
    "The transformer was originally introduced for a machine translation task. Below is the figure from original paper \"Attention Is All You Need\" (2017)\n",
    "\n",
    "![figs/attention.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/attention.png?raw=1)\n",
    "\n",
    "\n",
    "### Back to BERT\n",
    "\n",
    "BERT is a \"multi-layer bidirectional Transformer encoder\"\n",
    "- to train, mask words at random so that information about word $i$ is not used when predicting word $i$\n",
    "\n",
    "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/bert.png?raw=1\" width=60%/>\n",
    "\n",
    "**Just the Encoder portion of the original Transformer architecture**\n",
    "\n",
    "### Back to ELMO\n",
    "\n",
    "ELMO \"word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.\"\n",
    "\n",
    "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/elmo.png?raw=1\" width=60%/>\n",
    "\n",
    "\n",
    "### GPT: Generative Pre-trained Transformer\n",
    "\n",
    "**Just the Decoder portion of the original Transformer architecture**\n",
    "\n",
    "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/gpt.png?raw=1\" width=60%/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjDmlNE2vHdA",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Transformer implementation\n",
    "\n",
    "Andrej Karpathy's excellent, simplified implementation of transformer: https://github.com/karpathy/minGPT\n",
    "\n",
    "Along with code walkthrough video: https://www.youtube.com/watch?v=kCc8FmEb1nY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1YwQFKzvHdB"
   },
   "source": [
    "## sources\n",
    "\n",
    "- https://web.stanford.edu/class/cs224n/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s39jZ-bT8PW-",
    "outputId": "53f37457-3bde-46ec-8d2a-b6b1929122c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python -V"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
