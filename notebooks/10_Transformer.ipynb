{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7t32OQMvRZq"
      },
      "source": [
        "![Tulane](https://github.com/tulane-cmps6730/main/blob/main/img/banner.png?raw=true)\n",
        "\n",
        "<center>\n",
        "\n",
        "<font size=\"+3\">Transformers</font>\n",
        "\n",
        "[Aron Culotta](https://cs.tulane.edu/~aculotta/)  \n",
        "[Tulane University](https://cs.tulane.edu/)\n",
        "\n",
        "<a href=\"http://colab.research.google.com/github/tulane-cmps6730/main/blob/main/notebooks/10_Transformers.ipynb\">\n",
        "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Google_Colaboratory_SVG_Logo.svg/320px-Google_Colaboratory_SVG_Logo.svg.png\"  width=10%/></a>\n",
        "<a href=\"https://github.com/tulane-cmps6730/main/tree/main\">\n",
        "        <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/GitHub_Invertocat_Logo.svg/240px-GitHub_Invertocat_Logo.svg.png\" width=6%/></a>\n",
        "\n",
        "In this module, we'll learn about the transformer language model, which uses attention to capture the long-range dependencies of language.\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8OiXeEVwfGw"
      },
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Motivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb8V5vvEvhNR"
      },
      "source": [
        "The GPT in ChatGPT stands for...\n",
        "\n",
        "- **G**enerative\n",
        "- **P**retrained\n",
        "- **T**ransformer\n",
        "\n",
        "Today we'll learn what this means."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnX9QdcvxEa5"
      },
      "source": [
        "### Attention for Classification Review\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np7Bt8nHvhNR"
      },
      "source": [
        "![figs/lstmclf4.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/lstmclf4.png?raw=1)\n",
        "\n",
        "\n",
        "> Given a set of vector **values**, and a vector **query**,  \n",
        "> **attention** is a technique to compute a weighted sum of the values, dependent on the query.\n",
        "\n",
        "- a selective summary of the values based on the query\n",
        "- gives a fixed-size representation of the values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwkgFJPUvHc9"
      },
      "source": [
        "\n",
        "**Input**: sequence of value vectors $\\mathbf{h}_1 \\ldots \\mathbf{h}_n \\in \\mathbb{R}^{d_h}$ and a query vector $\\mathbf{q} \\in \\mathbb{R}^{d_q}$\n",
        "\n",
        "1. Compute **attention scores** $\\mathbf{s} \\in \\mathbb{R}^n$\n",
        "  - we'll see how in a moment\n",
        "\n",
        "\n",
        "2. Apply softmax to get the **attention distribution** $\\alpha$:\n",
        "  - $\\alpha = \\mathrm{softmax}(\\mathbf{s}) \\in \\mathbb{R}^n$\n",
        "  \n",
        "  \n",
        "3. Compute the **attention output**, the sum of values weighted by attention distribution:\n",
        "  - $\\mathbf{a} = \\sum_i^n \\alpha_i \\mathbf{h}_i \\in \\mathbb{R}^{d_h}$\n",
        "  - $\\mathbf{a}$ then becomes input features for classification layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioD31_UvxUbm"
      },
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Self-Attention for Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overview"
      ],
      "metadata": {
        "id": "f5fR8nbqFtoZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnX9hjiXvHc9"
      },
      "source": [
        "\n",
        "- We can use the idea of attention to predict the next word in a sentence.\n",
        "- Attention lets us model long-range dependencies\n",
        "- The prediction for word $i$ depends on **all previous words**\n",
        "\n",
        "\n",
        "\n",
        "![figs/m10selfatt.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10selfatt.png?raw=1)\n",
        "\n",
        "![figs/m10selfatt2.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10selfatt2.png?raw=1)\n",
        "\n",
        "We can use attention to determine which other words are important to predict the next word $t$.\n",
        "\n",
        "$$\\mathbf{a}_i =  \\sum_{j \\le i}^n \\alpha_{ij} \\mathbf{v}_j$$\n",
        "\n",
        "\n",
        "\n",
        "Each input embedding plays three different roles:\n",
        "\n",
        "- As the current focus of attention when being compared to all of the other preceding inputs. We’ll refer to this role as a **query**.\n",
        "- As a preceding input being compared to the current focus of attention. We’ll refer to this role as a **key**.\n",
        "- As a **value** used to compute the output for the current focus of attention.\n",
        "\n",
        "We use three different weight matrices to represent each role.\n",
        "\n",
        "$\\mathbf{v}_j = W_v\\mathbf{x_j} ~~~W_v\\in \\mathbb{R}^{dxd} ~~~$ **values for node j**\n",
        "\n",
        "$\\mathbf{k}_j = W_k \\mathbf{x}_j ~~~W_k\\in \\mathbb{R}^{dxd} ~~~$ **keys for node j**\n",
        "\n",
        "$\\mathbf{q}_i = W_q\\mathbf{x_i}~~~$ **query for node i**\n",
        "\n",
        "$\\alpha_{ij} = \\frac{\\exp(\\mathbf{q}_i \\cdot \\mathbf{k}_j)}{\\sum_{j'} \\exp({\\mathbf{q}_i \\cdot \\mathbf{k}_{j'})}} ~~~$ **affinities between node i and j**\n",
        "\n",
        "**nb:** we often scale down dot product for better numerical stability:\n",
        "$\\alpha_{ij} = \\frac{\\exp(\\mathbf{q}_i \\cdot \\mathbf{k}_j) / \\sqrt{d_k}}{\\sum_{j'} \\exp({\\mathbf{q}_i \\cdot \\mathbf{k}_{j'})}}$\n",
        "\n",
        "**Putting it all together:**\n",
        "\n",
        "![figs/m10selfatt3.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10selfatt3.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embeddings and Unembeddings\n"
      ],
      "metadata": {
        "id": "gw4vuJPBFyib"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlVosxO5vhNT"
      },
      "source": [
        "\n",
        "To use self-attention for language modeling, we need two additional layers.\n",
        "- **Embedding layer**: from one-hot encoding of input word to a dense word embedding vector\n",
        "\n",
        "\n",
        "![figs/m10selfatt2.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10embed.png?raw=1)\n",
        "\n",
        "$$\\mathbf{x}_i \\times E = \\mathbf{e}_i$$\n",
        "\n",
        "$$ \\begin{bmatrix}0&1&0&0\\end{bmatrix} \\times \\begin{bmatrix}.1&.2\\\\.3&.4\\\\-.1&-.2\\\\.5&.5\\end{bmatrix}  = \\begin{bmatrix}.3&.4\\end{bmatrix}$$\n",
        "\n",
        "<br><br>\n",
        "Finally, for the output of the model, we need a way to convert from this embedding space back to the vocabulary space.\n",
        "\n",
        "- **Unembedding layer**: from a dense word embedding vector to a distribution over output words\n",
        "\n",
        "Can use $E^T$ to convert from output of attention layer to logits over word outputs.\n",
        "\n",
        "$$ \\mathbf{u}_i = \\mathbf{a}_i E^T$$\n",
        "\n",
        "$$ \\begin{bmatrix}.3&.4\\end{bmatrix} \\times \\begin{bmatrix}.1&.3&-.1&.5\\\\.2&.4&-.2&.5\\end{bmatrix}  = \\begin{bmatrix}.11&.25&-.11&.35\\end{bmatrix}$$\n",
        "\n",
        "Then take softmax to get distribution over words in vocabulary:\n",
        "\n",
        "$$\\mathbf{y_i} = \\mathrm{softmax}(\\mathbf{u}_i)$$\n",
        "\n",
        "\\begin{bmatrix} 0.2367 & 0.2723 & 0.1900 & 0.3010 \\end{bmatrix}\n",
        "\n",
        "Thus, the most likely next word in this example is the fourth word in the vocabulary (Pr=.3010)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Efficient multiplication\n"
      ],
      "metadata": {
        "id": "q0SFcge4F0pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Rather than computing attention one word at a time, we can construct a matrix of all word embeddings, and perform matrix multiplications to get matrices $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$. This is much more efficient than a series of vector-matrix multiplications, since this operations can be highly parallelized in GPUs.\n",
        "\n",
        "To do so:\n",
        "$$\n",
        "Q = XW_q ; K = XW_k ; V = XW_v\n",
        "$$\n",
        "\n",
        "Then, to perform query-key comparisons, another matrix multiplication can be used:\n",
        "\n",
        "$$QK^t$$\n",
        "\n",
        "The final attention values matrix then becomes:\n",
        "\n",
        "$$A = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
      ],
      "metadata": {
        "id": "wPTqCYtl4m8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Self-Attention Example\n",
        "Let's work through an example computing attention for the sentence \"a a b c\"\n",
        "\n",
        "- We'll assume an embedding of size 2"
      ],
      "metadata": {
        "id": "kG6-f1S385d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "Assume three words in the vocabulary (a=0,b=1,c=2).\n",
        "One-hot encoding of the sentence \"a a b c\" is:\n",
        "\"\"\"\n",
        "# 1. Create one-hot encoddings of sentence.\n",
        "word1 = torch.tensor([1.,0.,0.])\n",
        "word2 = torch.tensor([1.,0.,0.])\n",
        "word3 = torch.tensor([0.,1.,0.])\n",
        "word4 = torch.tensor([0.,0.,1.])\n",
        "sentence = torch.vstack([word1,word2,word3, word4])\n",
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1Go7f_347ID",
        "outputId": "a5b8107c-0cc1-4bc3-bc08-5651a8985510"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create word embedding matrix X\n",
        "\n",
        "# Assume our embeddings are size 2.\n",
        "# Let's use random values for the embedding.\n",
        "# E has dimension V x d\n",
        "# V=vocab size   d=embedding dimension\n",
        "# So, row i is the embedding for word i\n",
        "E = torch.tensor([[.1,.2],[.3,.4],[-.1,-.2]])\n",
        "E"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa1beI2Q5aSQ",
        "outputId": "86eed33e-b825-4f4c-9555-cfd798a60295"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1000,  0.2000],\n",
              "        [ 0.3000,  0.4000],\n",
              "        [-0.1000, -0.2000]])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Compute embeddings for entire sentence in one matrix multiplication.\n",
        "sentence_embeddings = sentence @ E  # <--- note matrix multiplication operator @\n",
        "sentence_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLE0bR7A5hwJ",
        "outputId": "78912cc7-c93d-4dea-97ea-fe5e8676fb3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1000,  0.2000],\n",
              "        [ 0.1000,  0.2000],\n",
              "        [ 0.3000,  0.4000],\n",
              "        [-0.1000, -0.2000]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Create attention weight matrices\n",
        "# W_q,W_k,W_v matrix all dxd\n",
        "torch.manual_seed(24)\n",
        "# random, but multiplying by scalars to get some larger values.\n",
        "W_q = torch.randn(2,2) * 5\n",
        "W_k = torch.randn(2,2) * 10\n",
        "W_v = torch.randn(2,2) * 3\n",
        "W_q, W_k, W_v"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iywm1e7Y6yr2",
        "outputId": "b63db5a6-62ca-4e93-fa81-9c05bac05dce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 5.0697,  4.4942],\n",
              "         [-1.0553, -7.6630]]),\n",
              " tensor([[ -6.1630,   2.2882],\n",
              "         [ -1.1195, -20.5059]]),\n",
              " tensor([[-1.8566, -2.3413],\n",
              "         [-2.7651,  1.4420]]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Compute Q,K,V matrices with matrix multiplication.\n",
        "Q = sentence_embeddings @ W_q\n",
        "K = sentence_embeddings @ W_k\n",
        "V = sentence_embeddings @ W_v\n",
        "Q,K,V"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_kx_7h_7Wb2",
        "outputId": "ed5fe70e-3893-4b45-f734-bd6974bb905d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.2959, -1.0832],\n",
              "         [ 0.2959, -1.0832],\n",
              "         [ 1.0988, -1.7170],\n",
              "         [-0.2959,  1.0832]]),\n",
              " tensor([[-0.8402, -3.8723],\n",
              "         [-0.8402, -3.8723],\n",
              "         [-2.2967, -7.5159],\n",
              "         [ 0.8402,  3.8723]]),\n",
              " tensor([[-0.7387,  0.0543],\n",
              "         [-0.7387,  0.0543],\n",
              "         [-1.6630, -0.1256],\n",
              "         [ 0.7387, -0.0543]]))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Compute similarity scores between all queries and keys.\n",
        "similarities = Q @ K.T\n",
        "similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOtiVUFL8sZ-",
        "outputId": "7873848b-e639-46bd-a916-ba5a269d1264"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.9459,  3.9459,  7.4615, -3.9459],\n",
              "        [ 3.9459,  3.9459,  7.4615, -3.9459],\n",
              "        [ 5.7255,  5.7255, 10.3810, -5.7255],\n",
              "        [-3.9459, -3.9459, -7.4615,  3.9459]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Compute softmax of similarities (alphas)\n",
        "from torch.nn.functional import softmax\n",
        "import math\n",
        "# divide by sqrt(dimension) to reduce scale of values\n",
        "sm = softmax(similarities/math.sqrt(2), dim=-1) # normalize by rows\n",
        "sm\n",
        "# So, value .0037 in last row means to create output for last word, the first word contributes .0037 of its value vector to the last word."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBiyo29y83YY",
        "outputId": "299a907d-67a6-45e3-9a35-466e2e9e2480"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7.1346e-02, 7.1346e-02, 8.5704e-01, 2.6907e-04],\n",
              "        [7.1346e-02, 7.1346e-02, 8.5704e-01, 2.6907e-04],\n",
              "        [3.4609e-02, 3.4609e-02, 9.3077e-01, 1.0536e-05],\n",
              "        [3.7420e-03, 3.7420e-03, 3.1151e-04, 9.9220e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Multiply by values.\n",
        "print('mutiplying similarity matrix Q*K^T (NxN)')\n",
        "print(sm)\n",
        "print('\\nby value matrix (V) [one row per word in sentence]')\n",
        "print(V)\n",
        "A = sm @ V\n",
        "print('\\n\\nresult (A) [one row per word in sentence]')\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENhz2GFU_AUG",
        "outputId": "b1201de4-7845-4266-f1df-2bdc557be355"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mutiplying similarity matrix Q*K^T (NxN)\n",
            "tensor([[7.1346e-02, 7.1346e-02, 8.5704e-01, 2.6907e-04],\n",
            "        [7.1346e-02, 7.1346e-02, 8.5704e-01, 2.6907e-04],\n",
            "        [3.4609e-02, 3.4609e-02, 9.3077e-01, 1.0536e-05],\n",
            "        [3.7420e-03, 3.7420e-03, 3.1151e-04, 9.9220e-01]])\n",
            "\n",
            "by value matrix (V) [one row per word in sentence]\n",
            "tensor([[-0.7387,  0.0543],\n",
            "        [-0.7387,  0.0543],\n",
            "        [-1.6630, -0.1256],\n",
            "        [ 0.7387, -0.0543]])\n",
            "\n",
            "\n",
            "result (A) [one row per word in sentence]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5305, -0.0999],\n",
              "        [-1.5305, -0.0999],\n",
              "        [-1.5990, -0.1132],\n",
              "        [ 0.7269, -0.0535]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Unembed\n",
        "logits = A @ E.T\n",
        "logits\n",
        "# result is N x d. One row for each word in sentence with scores for possible words in that location."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghJ-eC9iAPYd",
        "outputId": "bddb855b-ccd6-496c-c898-4180ef01ce9f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1730, -0.4991,  0.1730],\n",
              "        [-0.1730, -0.4991,  0.1730],\n",
              "        [-0.1825, -0.5250,  0.1825],\n",
              "        [ 0.0620,  0.1967, -0.0620]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. Softmax to get probability of each word in the sentence.\n",
        "probas = softmax(logits, dim=-1)\n",
        "probas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmrOeysEAfMx",
        "outputId": "592f33cf-4272-4e84-8435-dce2368dcd55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3190, 0.2302, 0.4508],\n",
              "        [0.3190, 0.2302, 0.4508],\n",
              "        [0.3174, 0.2254, 0.4572],\n",
              "        [0.3303, 0.3779, 0.2918]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Pick top prediction for each element.\n",
        "vocabulary = np.array([\"a\", \"b\", \"c\"])\n",
        "topi = probas.argmax(dim=-1)\n",
        "most_probable_words = vocabulary[topi]\n",
        "most_probable_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hknJPvWAp7t",
        "outputId": "4373f9cb-f5f3-49b6-ec05-02affe35eeb3"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['c', 'c', 'c', 'b'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using only the past to predict the future"
      ],
      "metadata": {
        "id": "zrxdpPuCIT_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "There was a problem in the above example\n",
        "\n",
        "```python\n",
        "similarities = Q @ K.T\n",
        "\n",
        "tensor([[ 3.9459,  3.9459,  7.4615, -3.9459],\n",
        "        [ 3.9459,  3.9459,  7.4615, -3.9459],\n",
        "        [ 5.7255,  5.7255, 10.3810, -5.7255],\n",
        "        [-3.9459, -3.9459, -7.4615,  3.9459]])\n",
        "```\n",
        "\n",
        "We have been assuming that word $i$ can only attend to **previous** words in the sentence (up to and including $i$):\n",
        "\n",
        "$$\\mathbf{a}_i =  \\sum_{j \\le i}^n \\alpha_{ij} \\mathbf{v}_j$$\n",
        "\n",
        "But, in the calculations above, word $i$ attended **all** the words in the sentence. (`Q @ K.T`)\n",
        "\n",
        "**Solution:** Just \"mask out\" the future values in the similarity matrix before computing softmax.\n",
        "\n",
        "![figs/m10mask.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10mask.png?raw=1)\n"
      ],
      "metadata": {
        "id": "EcA6NlwQCqol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute new similarity matrix QK^T while keeping future values at -inf\n",
        "# triu = upper trianglular\n",
        "# diagonal=1 === keep diagonal\n",
        "similarities + torch.triu(torch.full_like(similarities, float('-inf')), diagonal=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jW2g_HvEQDW",
        "outputId": "8536b14c-7b98-4df9-a23d-b15de603b1e7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 3.9459,    -inf,    -inf,    -inf],\n",
              "        [ 3.9459,  3.9459,    -inf,    -inf],\n",
              "        [ 5.7255,  5.7255, 10.3810,    -inf],\n",
              "        [-3.9459, -3.9459, -7.4615,  3.9459]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# when we softmax, the -inf becomes 0\n",
        "def mask_future(similarities):\n",
        "    return similarities + torch.triu(torch.full_like(similarities, float('-inf')), diagonal=1)\n",
        "\n",
        "sm = softmax(mask_future(similarities)/math.sqrt(2), dim=-1) # normalize by rows\n",
        "sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8SBRlJpEj-S",
        "outputId": "6a974ff1-e8d9-4ece-8591-b8ecc34506a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
              "        [5.0000e-01, 5.0000e-01, 0.0000e+00, 0.0000e+00],\n",
              "        [3.4609e-02, 3.4609e-02, 9.3078e-01, 0.0000e+00],\n",
              "        [3.7420e-03, 3.7420e-03, 3.1151e-04, 9.9220e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# doesn't actually change prediction in this example ;)\n",
        "A = sm @ V\n",
        "logits = A @ E.T\n",
        "probas = softmax(logits, dim=-1)\n",
        "most_probable_words = vocabulary[probas.argmax(dim=-1)]\n",
        "most_probable_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Xpnx9PpExLV",
        "outputId": "ddbe71ad-4fe1-4a34-a110-0ab6454a6dc1"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['c', 'c', 'c', 'b'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "P = nn.Linear(2, 4, bias=False)\n",
        "torch.eye(4) @ P.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOZVwxrzyfk5",
        "outputId": "9ef2fac2-f4d8-49a4-cea6-cf7978fc295d"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5406,  0.5869],\n",
              "        [-0.1657,  0.6496],\n",
              "        [-0.1549,  0.1427],\n",
              "        [-0.3443,  0.4153]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Implementation: Simple Attention-based Language Model"
      ],
      "metadata": {
        "id": "kjEk0qGS175l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "5uZKf05kvhNT"
      },
      "outputs": [],
      "source": [
        "# Let's package these computations up inside of an nn.Module.\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SimpleSelfAttention(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_size, seq_len,\n",
        "               use_positional_embedding=True, verbose=False):\n",
        "    super(SimpleSelfAttention, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.vocab_size = vocab_size\n",
        "    self.seq_len = seq_len\n",
        "    self.verbose = verbose\n",
        "    self.use_positional_embedding = use_positional_embedding\n",
        "\n",
        "    # word embedding and unembedding\n",
        "    self.E = nn.Linear(embedding_size, vocab_size, bias=False)\n",
        "\n",
        "    # position embedding and unembedding\n",
        "    self.P = nn.Linear(embedding_size, seq_len, bias=False)\n",
        "\n",
        "    # attention weights\n",
        "    # values\n",
        "    self.W_v = nn.Linear(embedding_size, embedding_size)\n",
        "    # keys\n",
        "    self.W_k = nn.Linear(embedding_size, embedding_size)\n",
        "    # queries\n",
        "    self.W_q = nn.Linear(embedding_size, embedding_size)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "\n",
        "\n",
        "  def mask_future(self, similarities):\n",
        "    return similarities + torch.triu(torch.full_like(similarities, float('-inf')), diagonal=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # using self. for these temporary values below so we can debug later on\n",
        "    # embed words\n",
        "    self.sentence_embeddings = x @ self.E.weight\n",
        "    if self.use_positional_embedding:\n",
        "      # embed positions\n",
        "      self.pos_embedding = torch.eye(self.seq_len) @ self.P.weight\n",
        "      # sum word and position embeddings\n",
        "      self.sentence_embeddings += self.pos_embedding\n",
        "    # Q,K,V\n",
        "    self.Q = self.W_q(self.sentence_embeddings)\n",
        "    self.K = self.W_k(self.sentence_embeddings)\n",
        "    self.V = self.W_v(self.sentence_embeddings)\n",
        "    self.similarities = self.Q @ self.K.T\n",
        "    # normalize by rows\n",
        "    self.sm = self.softmax(self.mask_future(self.similarities)/math.sqrt(self.embedding_size))\n",
        "    self.A = self.sm @ self.V\n",
        "    self.logits = self.A @ self.E.weight.T\n",
        "    # leave as logits for crossentropy loss later.\n",
        "    if self.verbose:\n",
        "      print('A matrix (words x embed_size)')\n",
        "      display(pd.DataFrame(\n",
        "              pp(self.A)\n",
        "              ))\n",
        "      print('logits (words x vocab_size)')\n",
        "      display(pd.DataFrame(\n",
        "              pp(logits)\n",
        "              ))\n",
        "    return self.logits\n",
        "\n",
        "def pp(arr):\n",
        "  # pretty print weight array\n",
        "  return arr.detach().cpu().numpy().round(decimals=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UHJC7XI8P-Q",
        "outputId": "bc8dfae8-8404-45fc-deda-8fe8d390f189"
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 0., 0.],\n",
              "        [0., 1., 0.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(24)\n",
        "ssa = SimpleSelfAttention(vocab_size=3, embedding_size=2, seq_len=4,\n",
        "                          use_positional_embedding=True, verbose=True)\n",
        "logits = ssa.forward(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "ASdGXqgcUvhH",
        "outputId": "0fe59c5e-5420-417e-9625-b1d5592c8359"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A matrix (words x embed_size)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      0     1\n",
              "0 -0.24 -0.22\n",
              "1 -0.14 -0.07\n",
              "2 -0.38 -0.30\n",
              "3 -0.29 -0.20"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20f8fab3-7a9e-4191-aa0d-e14e3e1abc78\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.24</td>\n",
              "      <td>-0.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.38</td>\n",
              "      <td>-0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.29</td>\n",
              "      <td>-0.20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20f8fab3-7a9e-4191-aa0d-e14e3e1abc78')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-20f8fab3-7a9e-4191-aa0d-e14e3e1abc78 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-20f8fab3-7a9e-4191-aa0d-e14e3e1abc78');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bf650e18-393b-4c04-aefb-00dbf840ab15\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf650e18-393b-4c04-aefb-00dbf840ab15')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bf650e18-393b-4c04-aefb-00dbf840ab15 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"logits = ssa\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.14000000059604645,\n          -0.28999999165534973,\n          -0.23999999463558197\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.07000000029802322,\n          -0.20000000298023224,\n          -0.2199999988079071\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits (words x vocab_size)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      0     1     2\n",
              "0 -0.05  0.14 -0.09\n",
              "1 -0.04  0.08 -0.07\n",
              "2 -0.09  0.21 -0.15\n",
              "3 -0.07  0.16 -0.13"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2e84564f-00db-4c15-a9db-ffcf8f64e833\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.14</td>\n",
              "      <td>-0.09</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.08</td>\n",
              "      <td>-0.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.09</td>\n",
              "      <td>0.21</td>\n",
              "      <td>-0.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.16</td>\n",
              "      <td>-0.13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e84564f-00db-4c15-a9db-ffcf8f64e833')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2e84564f-00db-4c15-a9db-ffcf8f64e833 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2e84564f-00db-4c15-a9db-ffcf8f64e833');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d320fd7d-a349-49dd-8feb-4b40b075c293\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d320fd7d-a349-49dd-8feb-4b40b075c293')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d320fd7d-a349-49dd-8feb-4b40b075c293 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"logits = ssa\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": 0,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.03999999910593033,\n          -0.07000000029802322,\n          -0.05000000074505806\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 1,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.07999999821186066,\n          0.1599999964237213,\n          0.14000000059604645\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2,\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          -0.07000000029802322,\n          -0.12999999523162842,\n          -0.09000000357627869\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**training code is same as in LSTM lecture**"
      ],
      "metadata": {
        "id": "G75m-1AaIi4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# same as our previous train_model functions...\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_model(model, data, epochs=20, learning_rate=0.1, batch_size=10):\n",
        "    \"\"\"\n",
        "    Iterate through the data in batches and train the model.\n",
        "    \"\"\"\n",
        "    torch.random.manual_seed(42)  # for reproducibility\n",
        "    np.random.seed(42)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate)\n",
        "    model.verbose = False\n",
        "    loss_val = []\n",
        "    # main training loop\n",
        "    for epoch in range(epochs):\n",
        "        print('epoch=%d' % epoch)\n",
        "        np.random.shuffle(data)\n",
        "        epoch_loss = 0\n",
        "        # for each batch of instances\n",
        "        with tqdm(total=len(data), desc=f'Epoch {epoch+1}/{epochs}', unit='sample') as pbar:\n",
        "          for i in range(0, len(data), batch_size):\n",
        "            batch = data[i:i + batch_size]\n",
        "            optimizer.zero_grad() # reset all the gradient information\n",
        "            batch_loss = 0\n",
        "            for datum in batch:\n",
        "                result = model.forward(datum[:-1])\n",
        "                loss = criterion(result,\n",
        "                                 datum[1:].argmax(dim=1).long())\n",
        "                batch_loss += loss\n",
        "            # change parameters after each batch\n",
        "            batch_loss /= batch_size\n",
        "            batch_loss.backward()       # computes all the gradients\n",
        "            optimizer.step()            # update parameters\n",
        "            loss_val.append(batch_loss.item())\n",
        "            epoch_loss += batch_loss\n",
        "            pbar.update(len(batch))\n",
        "            pbar.set_postfix({'Batch': (i // batch_size) + 1, 'Loss': batch_loss.item()})\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / (len(data) / batch_size)\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Average Loss: {avg_epoch_loss:.4f}')\n",
        "\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(loss_val, 'bo-')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('batches')\n",
        "    plt.show()\n",
        "    return model"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zJYYBDPrV76o"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on \"abc\" example"
      ],
      "metadata": {
        "id": "EjJgX0Db1898"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def char2vec(char, char2int):\n",
        "    x = torch.zeros((1, len(char2int)))\n",
        "    x[0][char2int[char]] = 1\n",
        "    return x\n",
        "\n",
        "def chars2vec(chars, char2int):\n",
        "    return torch.cat([char2vec(c, char2int) for c in chars])\n",
        "\n",
        "chars = ['a', 'b', 'c']\n",
        "char2int = {'a': 0, 'b': 1, 'c': 2}\n",
        "int2char = {0: 'a', 1: 'b', 2: 'c'}\n",
        "data = [\n",
        "    chars2vec('abcabcabc', char2int),\n",
        "    chars2vec('bcabcabca', char2int),\n",
        "    chars2vec('cabcabcab', char2int)\n",
        "]  * 100\n",
        "\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Te5T0BxQYRO8",
        "outputId": "70e4fd7f-9b0d-4123-e19d-4e7c3ecd8d36"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(24)\n",
        "ssa = SimpleSelfAttention(vocab_size=3, embedding_size=2, seq_len=8,\n",
        "                          use_positional_embedding=True, verbose=True)\n",
        "train_model(ssa, data, epochs=2, learning_rate=.2, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "XEllvLb-Ybfa",
        "outputId": "40e62c87-0f05-4f3b-d6f7-2bac3e5d005d"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/2: 100%|██████████| 300/300 [00:00<00:00, 758.94sample/s, Batch=30, Loss=0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Average Loss: 0.5075\n",
            "epoch=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/2: 100%|██████████| 300/300 [00:00<00:00, 515.42sample/s, Batch=30, Loss=0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/2], Average Loss: 0.0000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleSelfAttention(\n",
              "  (E): Linear(in_features=2, out_features=3, bias=False)\n",
              "  (P): Linear(in_features=2, out_features=8, bias=False)\n",
              "  (W_v): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (W_k): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (W_q): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = chars2vec('abcabcabc', char2int)\n",
        "outputs = ssa.forward(x[:-1])\n",
        "vocabulary[outputs.argmax(axis=1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cievvTB31RDc",
        "outputId": "a7c9e72c-6fdb-4a9f-ac2e-deb5490b75fe"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['b', 'c', 'a', 'b', 'c', 'a', 'b', 'c'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ssa.E.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deHfN6SAwu5-",
        "outputId": "528bfd98-2ef1-4eb5-86a2-c0bc7c464e3b"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[-2.7532,  2.0580],\n",
              "        [-1.4466, -2.6530],\n",
              "        [ 3.8879,  0.0807]], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing on long-range example"
      ],
      "metadata": {
        "id": "bhko_iM42BGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's revisit the synthetic task from LSTM lecture. We observe two types of sequences:\n",
        "\n",
        "> 'bc**a**bcbcbc**a**'\n",
        "\n",
        "> 'bcbcbcbcbc'\n",
        "\n",
        "The network needs to learn that when it sees an 'a', it must remember this in order to output another 'a' 7 time steps later."
      ],
      "metadata": {
        "id": "dOrrd1gI0sVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = [\n",
        "    chars2vec('bcabcbcbca', char2int),\n",
        "    chars2vec('bcbcbcbcbc', char2int),\n",
        "]  * 10\n",
        "\n",
        "ssa2 = SimpleSelfAttention(vocab_size=3, embedding_size=2, seq_len=9,\n",
        "                           use_positional_embedding=True, verbose=True)\n",
        "train_model(ssa2, data2, epochs=10, learning_rate=.1, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "amAtoJcBxVxW",
        "outputId": "447ce95e-3ac9-47f4-dbd3-8d6689778123"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|██████████| 20/20 [00:00<00:00, 458.72sample/s, Batch=2, Loss=0.982]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Average Loss: 1.0025\n",
            "epoch=1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 20/20 [00:00<00:00, 513.75sample/s, Batch=2, Loss=0.935]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Average Loss: 0.9226\n",
            "epoch=2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 20/20 [00:00<00:00, 516.64sample/s, Batch=2, Loss=0.811]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Average Loss: 0.8645\n",
            "epoch=3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 20/20 [00:00<00:00, 512.36sample/s, Batch=2, Loss=0.648]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Average Loss: 0.7218\n",
            "epoch=4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 20/20 [00:00<00:00, 445.60sample/s, Batch=2, Loss=0.375]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Average Loss: 0.4791\n",
            "epoch=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 20/20 [00:00<00:00, 491.60sample/s, Batch=2, Loss=0.21]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Average Loss: 0.2426\n",
            "epoch=6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 20/20 [00:00<00:00, 529.90sample/s, Batch=2, Loss=0.177]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Average Loss: 0.1926\n",
            "epoch=7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 20/20 [00:00<00:00, 498.18sample/s, Batch=2, Loss=0.123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Average Loss: 0.1220\n",
            "epoch=8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 20/20 [00:00<00:00, 555.38sample/s, Batch=2, Loss=0.156]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Average Loss: 0.1300\n",
            "epoch=9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 20/20 [00:00<00:00, 471.73sample/s, Batch=2, Loss=0.102]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Average Loss: 0.1123\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleSelfAttention(\n",
              "  (E): Linear(in_features=2, out_features=3, bias=False)\n",
              "  (P): Linear(in_features=2, out_features=9, bias=False)\n",
              "  (W_v): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (W_k): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (W_q): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predct \"a\" in last position?\n",
        "x = chars2vec('bcabcbcbca', char2int)\n",
        "outputs = ssa2.forward(x[:-1])\n",
        "vocabulary[outputs.argmax(axis=1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szqPAJbkxi_w",
        "outputId": "035ab40e-d6d6-4d28-d9db-a2327d7dad55"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['c', 'b', 'b', 'c', 'b', 'c', 'b', 'c', 'a'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Index(chars[:-1], name='input')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRhAhGbL585M",
        "outputId": "f4271b5c-f6b4-49c6-b22a-474a53e96b41"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['b', 'c', 'a', 'b', 'c', 'b', 'c', 'b', 'c'], dtype='object', name='input')"
            ]
          },
          "metadata": {},
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Examining attention weights\n",
        "\n",
        "**Can we see the model \"paying attention\" to the \"c\" to predict \"a\" at the end??**"
      ],
      "metadata": {
        "id": "oOvA-0Yd6swW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# examine the softmax attention weights (sm)\n",
        "inputs = list('bcabcbcbca')\n",
        "index = pd.Index(['%d_%s' % (i, char) for i, char in enumerate(inputs[:-1])], name='input')\n",
        "att = pd.DataFrame(ssa2.sm.detach().numpy(), index=index)\n",
        "att.columns = ['%d_%s' % (i, char) for i, char in enumerate(inputs[:-1])]\n",
        "# Convert tensor to Pandas DataFrame\n",
        "att.style.background_gradient(cmap='viridis')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "xw8Nvxp12auR",
        "outputId": "48ae2a1d-c8c6-42b4-efc2-324962ef556a"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7bc558c92a40>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_e2fa2_row0_col0, #T_e2fa2_row1_col1, #T_e2fa2_row2_col2, #T_e2fa2_row3_col0, #T_e2fa2_row4_col3, #T_e2fa2_row5_col0, #T_e2fa2_row6_col2, #T_e2fa2_row7_col5, #T_e2fa2_row7_col7, #T_e2fa2_row8_col4, #T_e2fa2_row8_col6, #T_e2fa2_row8_col8 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_e2fa2_row0_col1, #T_e2fa2_row0_col2, #T_e2fa2_row0_col3, #T_e2fa2_row0_col4, #T_e2fa2_row0_col5, #T_e2fa2_row0_col6, #T_e2fa2_row0_col7, #T_e2fa2_row0_col8, #T_e2fa2_row1_col0, #T_e2fa2_row1_col2, #T_e2fa2_row1_col3, #T_e2fa2_row1_col4, #T_e2fa2_row1_col5, #T_e2fa2_row1_col6, #T_e2fa2_row1_col7, #T_e2fa2_row1_col8, #T_e2fa2_row2_col0, #T_e2fa2_row2_col1, #T_e2fa2_row2_col3, #T_e2fa2_row2_col4, #T_e2fa2_row2_col5, #T_e2fa2_row2_col6, #T_e2fa2_row2_col7, #T_e2fa2_row2_col8, #T_e2fa2_row3_col1, #T_e2fa2_row3_col2, #T_e2fa2_row3_col4, #T_e2fa2_row3_col5, #T_e2fa2_row3_col6, #T_e2fa2_row3_col7, #T_e2fa2_row3_col8, #T_e2fa2_row4_col0, #T_e2fa2_row4_col5, #T_e2fa2_row4_col6, #T_e2fa2_row4_col7, #T_e2fa2_row4_col8, #T_e2fa2_row5_col1, #T_e2fa2_row5_col2, #T_e2fa2_row5_col4, #T_e2fa2_row5_col6, #T_e2fa2_row5_col7, #T_e2fa2_row5_col8, #T_e2fa2_row6_col0, #T_e2fa2_row6_col1, #T_e2fa2_row6_col4, #T_e2fa2_row6_col5, #T_e2fa2_row6_col6, #T_e2fa2_row6_col7, #T_e2fa2_row6_col8, #T_e2fa2_row7_col2, #T_e2fa2_row7_col8, #T_e2fa2_row8_col0, #T_e2fa2_row8_col2, #T_e2fa2_row8_col7 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row3_col3 {\n",
              "  background-color: #70cf57;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_e2fa2_row4_col1, #T_e2fa2_row8_col5 {\n",
              "  background-color: #440256;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row4_col2 {\n",
              "  background-color: #fbe723;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_e2fa2_row4_col4 {\n",
              "  background-color: #46075a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row5_col3 {\n",
              "  background-color: #453581;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row5_col5 {\n",
              "  background-color: #404588;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row6_col3 {\n",
              "  background-color: #460a5d;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row7_col0 {\n",
              "  background-color: #2c738e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row7_col1 {\n",
              "  background-color: #450559;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row7_col3 {\n",
              "  background-color: #2a778e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row7_col4 {\n",
              "  background-color: #277f8e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row7_col6 {\n",
              "  background-color: #472c7a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_e2fa2_row8_col1 {\n",
              "  background-color: #5ac864;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_e2fa2_row8_col3 {\n",
              "  background-color: #481a6c;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_e2fa2\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e2fa2_level0_col0\" class=\"col_heading level0 col0\" >0_b</th>\n",
              "      <th id=\"T_e2fa2_level0_col1\" class=\"col_heading level0 col1\" >1_c</th>\n",
              "      <th id=\"T_e2fa2_level0_col2\" class=\"col_heading level0 col2\" >2_a</th>\n",
              "      <th id=\"T_e2fa2_level0_col3\" class=\"col_heading level0 col3\" >3_b</th>\n",
              "      <th id=\"T_e2fa2_level0_col4\" class=\"col_heading level0 col4\" >4_c</th>\n",
              "      <th id=\"T_e2fa2_level0_col5\" class=\"col_heading level0 col5\" >5_b</th>\n",
              "      <th id=\"T_e2fa2_level0_col6\" class=\"col_heading level0 col6\" >6_c</th>\n",
              "      <th id=\"T_e2fa2_level0_col7\" class=\"col_heading level0 col7\" >7_b</th>\n",
              "      <th id=\"T_e2fa2_level0_col8\" class=\"col_heading level0 col8\" >8_c</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >input</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row0\" class=\"row_heading level0 row0\" >0_b</th>\n",
              "      <td id=\"T_e2fa2_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row1\" class=\"row_heading level0 row1\" >1_c</th>\n",
              "      <td id=\"T_e2fa2_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row2\" class=\"row_heading level0 row2\" >2_a</th>\n",
              "      <td id=\"T_e2fa2_row2_col0\" class=\"data row2 col0\" >0.000001</td>\n",
              "      <td id=\"T_e2fa2_row2_col1\" class=\"data row2 col1\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row2_col2\" class=\"data row2 col2\" >0.999999</td>\n",
              "      <td id=\"T_e2fa2_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row3\" class=\"row_heading level0 row3\" >3_b</th>\n",
              "      <td id=\"T_e2fa2_row3_col0\" class=\"data row3 col0\" >0.999771</td>\n",
              "      <td id=\"T_e2fa2_row3_col1\" class=\"data row3 col1\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row3_col2\" class=\"data row3 col2\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row3_col3\" class=\"data row3 col3\" >0.000228</td>\n",
              "      <td id=\"T_e2fa2_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row3_col8\" class=\"data row3 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row4\" class=\"row_heading level0 row4\" >4_c</th>\n",
              "      <td id=\"T_e2fa2_row4_col0\" class=\"data row4 col0\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row4_col1\" class=\"data row4 col1\" >0.006434</td>\n",
              "      <td id=\"T_e2fa2_row4_col2\" class=\"data row4 col2\" >0.992655</td>\n",
              "      <td id=\"T_e2fa2_row4_col3\" class=\"data row4 col3\" >0.000291</td>\n",
              "      <td id=\"T_e2fa2_row4_col4\" class=\"data row4 col4\" >0.000620</td>\n",
              "      <td id=\"T_e2fa2_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row4_col8\" class=\"data row4 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row5\" class=\"row_heading level0 row5\" >5_b</th>\n",
              "      <td id=\"T_e2fa2_row5_col0\" class=\"data row5 col0\" >0.996653</td>\n",
              "      <td id=\"T_e2fa2_row5_col1\" class=\"data row5 col1\" >0.000026</td>\n",
              "      <td id=\"T_e2fa2_row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row5_col3\" class=\"data row5 col3\" >0.000045</td>\n",
              "      <td id=\"T_e2fa2_row5_col4\" class=\"data row5 col4\" >0.000143</td>\n",
              "      <td id=\"T_e2fa2_row5_col5\" class=\"data row5 col5\" >0.003133</td>\n",
              "      <td id=\"T_e2fa2_row5_col6\" class=\"data row5 col6\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row5_col7\" class=\"data row5 col7\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row5_col8\" class=\"data row5 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row6\" class=\"row_heading level0 row6\" >6_c</th>\n",
              "      <td id=\"T_e2fa2_row6_col0\" class=\"data row6 col0\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row6_col1\" class=\"data row6 col1\" >0.000503</td>\n",
              "      <td id=\"T_e2fa2_row6_col2\" class=\"data row6 col2\" >0.999412</td>\n",
              "      <td id=\"T_e2fa2_row6_col3\" class=\"data row6 col3\" >0.000007</td>\n",
              "      <td id=\"T_e2fa2_row6_col4\" class=\"data row6 col4\" >0.000018</td>\n",
              "      <td id=\"T_e2fa2_row6_col5\" class=\"data row6 col5\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row6_col6\" class=\"data row6 col6\" >0.000060</td>\n",
              "      <td id=\"T_e2fa2_row6_col7\" class=\"data row6 col7\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row6_col8\" class=\"data row6 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row7\" class=\"row_heading level0 row7\" >7_b</th>\n",
              "      <td id=\"T_e2fa2_row7_col0\" class=\"data row7 col0\" >0.380413</td>\n",
              "      <td id=\"T_e2fa2_row7_col1\" class=\"data row7 col1\" >0.013113</td>\n",
              "      <td id=\"T_e2fa2_row7_col2\" class=\"data row7 col2\" >0.000000</td>\n",
              "      <td id=\"T_e2fa2_row7_col3\" class=\"data row7 col3\" >0.000116</td>\n",
              "      <td id=\"T_e2fa2_row7_col4\" class=\"data row7 col4\" >0.015945</td>\n",
              "      <td id=\"T_e2fa2_row7_col5\" class=\"data row7 col5\" >0.015243</td>\n",
              "      <td id=\"T_e2fa2_row7_col6\" class=\"data row7 col6\" >0.020024</td>\n",
              "      <td id=\"T_e2fa2_row7_col7\" class=\"data row7 col7\" >0.555145</td>\n",
              "      <td id=\"T_e2fa2_row7_col8\" class=\"data row7 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2fa2_level0_row8\" class=\"row_heading level0 row8\" >8_c</th>\n",
              "      <td id=\"T_e2fa2_row8_col0\" class=\"data row8 col0\" >0.000001</td>\n",
              "      <td id=\"T_e2fa2_row8_col1\" class=\"data row8 col1\" >0.743515</td>\n",
              "      <td id=\"T_e2fa2_row8_col2\" class=\"data row8 col2\" >0.000116</td>\n",
              "      <td id=\"T_e2fa2_row8_col3\" class=\"data row8 col3\" >0.000020</td>\n",
              "      <td id=\"T_e2fa2_row8_col4\" class=\"data row8 col4\" >0.037162</td>\n",
              "      <td id=\"T_e2fa2_row8_col5\" class=\"data row8 col5\" >0.000078</td>\n",
              "      <td id=\"T_e2fa2_row8_col6\" class=\"data row8 col6\" >0.163638</td>\n",
              "      <td id=\"T_e2fa2_row8_col7\" class=\"data row8 col7\" >0.000560</td>\n",
              "      <td id=\"T_e2fa2_row8_col8\" class=\"data row8 col8\" >0.054910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# predict \"c\" in last position?\n",
        "x = chars2vec('bcbcbcbcbc', char2int)\n",
        "outputs = ssa2.forward(x[:-1])\n",
        "vocabulary[outputs.argmax(axis=1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQUgh1wp0lxe",
        "outputId": "0caa3d3b-bdb2-450d-9d15-6601ee2e37a5"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['c', 'b', 'c', 'b', 'c', 'b', 'c', 'b', 'c'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# examine the softmax attention weights (sm)\n",
        "inputs = list('bcbcbcbcbc')\n",
        "index = pd.Index(['%d_%s' % (i, char) for i, char in enumerate(inputs[:-1])], name='input')\n",
        "att = pd.DataFrame(ssa2.sm.detach().numpy(), index=index)\n",
        "att.columns = ['%d_%s' % (i, char) for i, char in enumerate(inputs[:-1])]\n",
        "# Convert tensor to Pandas DataFrame\n",
        "att.style.background_gradient(cmap='viridis')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "TPjgXFpO7lez",
        "outputId": "bbcb5ce5-9495-432e-dd33-8eec613fb099"
      },
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7bc558c930a0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_7640e_row0_col0, #T_7640e_row1_col1, #T_7640e_row2_col0, #T_7640e_row2_col2, #T_7640e_row3_col3, #T_7640e_row5_col3, #T_7640e_row6_col4, #T_7640e_row6_col6, #T_7640e_row7_col5, #T_7640e_row7_col7, #T_7640e_row8_col8 {\n",
              "  background-color: #fde725;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_7640e_row0_col1, #T_7640e_row0_col2, #T_7640e_row0_col3, #T_7640e_row0_col4, #T_7640e_row0_col5, #T_7640e_row0_col6, #T_7640e_row0_col7, #T_7640e_row0_col8, #T_7640e_row1_col0, #T_7640e_row1_col2, #T_7640e_row1_col3, #T_7640e_row1_col4, #T_7640e_row1_col5, #T_7640e_row1_col6, #T_7640e_row1_col7, #T_7640e_row1_col8, #T_7640e_row2_col1, #T_7640e_row2_col3, #T_7640e_row2_col4, #T_7640e_row2_col5, #T_7640e_row2_col6, #T_7640e_row2_col7, #T_7640e_row2_col8, #T_7640e_row3_col0, #T_7640e_row3_col1, #T_7640e_row3_col4, #T_7640e_row3_col5, #T_7640e_row3_col6, #T_7640e_row3_col7, #T_7640e_row3_col8, #T_7640e_row4_col1, #T_7640e_row4_col3, #T_7640e_row4_col5, #T_7640e_row4_col6, #T_7640e_row4_col7, #T_7640e_row4_col8, #T_7640e_row5_col0, #T_7640e_row5_col1, #T_7640e_row5_col4, #T_7640e_row5_col6, #T_7640e_row5_col7, #T_7640e_row5_col8, #T_7640e_row6_col1, #T_7640e_row6_col3, #T_7640e_row6_col7, #T_7640e_row6_col8, #T_7640e_row7_col0, #T_7640e_row7_col1, #T_7640e_row7_col4, #T_7640e_row7_col6, #T_7640e_row7_col8, #T_7640e_row8_col0, #T_7640e_row8_col1, #T_7640e_row8_col2, #T_7640e_row8_col3, #T_7640e_row8_col4, #T_7640e_row8_col5, #T_7640e_row8_col6, #T_7640e_row8_col7 {\n",
              "  background-color: #440154;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row3_col2 {\n",
              "  background-color: #21a585;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row4_col0 {\n",
              "  background-color: #cde11d;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_7640e_row4_col2 {\n",
              "  background-color: #46075a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row4_col4 {\n",
              "  background-color: #2f6b8e;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row5_col2 {\n",
              "  background-color: #3d4d8a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row5_col5 {\n",
              "  background-color: #1f998a;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row6_col0 {\n",
              "  background-color: #1f988b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row6_col2 {\n",
              "  background-color: #40bd72;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row6_col5 {\n",
              "  background-color: #481467;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row7_col2 {\n",
              "  background-color: #450457;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_7640e_row7_col3 {\n",
              "  background-color: #fbe723;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_7640e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_7640e_level0_col0\" class=\"col_heading level0 col0\" >0_b</th>\n",
              "      <th id=\"T_7640e_level0_col1\" class=\"col_heading level0 col1\" >1_c</th>\n",
              "      <th id=\"T_7640e_level0_col2\" class=\"col_heading level0 col2\" >2_b</th>\n",
              "      <th id=\"T_7640e_level0_col3\" class=\"col_heading level0 col3\" >3_c</th>\n",
              "      <th id=\"T_7640e_level0_col4\" class=\"col_heading level0 col4\" >4_b</th>\n",
              "      <th id=\"T_7640e_level0_col5\" class=\"col_heading level0 col5\" >5_c</th>\n",
              "      <th id=\"T_7640e_level0_col6\" class=\"col_heading level0 col6\" >6_b</th>\n",
              "      <th id=\"T_7640e_level0_col7\" class=\"col_heading level0 col7\" >7_c</th>\n",
              "      <th id=\"T_7640e_level0_col8\" class=\"col_heading level0 col8\" >8_b</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th class=\"index_name level0\" >input</th>\n",
              "      <th class=\"blank col0\" >&nbsp;</th>\n",
              "      <th class=\"blank col1\" >&nbsp;</th>\n",
              "      <th class=\"blank col2\" >&nbsp;</th>\n",
              "      <th class=\"blank col3\" >&nbsp;</th>\n",
              "      <th class=\"blank col4\" >&nbsp;</th>\n",
              "      <th class=\"blank col5\" >&nbsp;</th>\n",
              "      <th class=\"blank col6\" >&nbsp;</th>\n",
              "      <th class=\"blank col7\" >&nbsp;</th>\n",
              "      <th class=\"blank col8\" >&nbsp;</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row0\" class=\"row_heading level0 row0\" >0_b</th>\n",
              "      <td id=\"T_7640e_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
              "      <td id=\"T_7640e_row0_col1\" class=\"data row0 col1\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row0_col5\" class=\"data row0 col5\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row0_col6\" class=\"data row0 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row0_col7\" class=\"data row0 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row0_col8\" class=\"data row0 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row1\" class=\"row_heading level0 row1\" >1_c</th>\n",
              "      <td id=\"T_7640e_row1_col0\" class=\"data row1 col0\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
              "      <td id=\"T_7640e_row1_col2\" class=\"data row1 col2\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row1_col4\" class=\"data row1 col4\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row1_col5\" class=\"data row1 col5\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row1_col6\" class=\"data row1 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row1_col7\" class=\"data row1 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row1_col8\" class=\"data row1 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row2\" class=\"row_heading level0 row2\" >2_b</th>\n",
              "      <td id=\"T_7640e_row2_col0\" class=\"data row2 col0\" >0.999723</td>\n",
              "      <td id=\"T_7640e_row2_col1\" class=\"data row2 col1\" >0.000001</td>\n",
              "      <td id=\"T_7640e_row2_col2\" class=\"data row2 col2\" >0.000277</td>\n",
              "      <td id=\"T_7640e_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row2_col4\" class=\"data row2 col4\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row2_col5\" class=\"data row2 col5\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row2_col6\" class=\"data row2 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row2_col7\" class=\"data row2 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row2_col8\" class=\"data row2 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row3\" class=\"row_heading level0 row3\" >3_c</th>\n",
              "      <td id=\"T_7640e_row3_col0\" class=\"data row3 col0\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row3_col1\" class=\"data row3 col1\" >0.000002</td>\n",
              "      <td id=\"T_7640e_row3_col2\" class=\"data row3 col2\" >0.000162</td>\n",
              "      <td id=\"T_7640e_row3_col3\" class=\"data row3 col3\" >0.999836</td>\n",
              "      <td id=\"T_7640e_row3_col4\" class=\"data row3 col4\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row3_col5\" class=\"data row3 col5\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row3_col6\" class=\"data row3 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row3_col7\" class=\"data row3 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row3_col8\" class=\"data row3 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row4\" class=\"row_heading level0 row4\" >4_b</th>\n",
              "      <td id=\"T_7640e_row4_col0\" class=\"data row4 col0\" >0.922738</td>\n",
              "      <td id=\"T_7640e_row4_col1\" class=\"data row4 col1\" >0.000015</td>\n",
              "      <td id=\"T_7640e_row4_col2\" class=\"data row4 col2\" >0.000005</td>\n",
              "      <td id=\"T_7640e_row4_col3\" class=\"data row4 col3\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row4_col4\" class=\"data row4 col4\" >0.077242</td>\n",
              "      <td id=\"T_7640e_row4_col5\" class=\"data row4 col5\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row4_col6\" class=\"data row4 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row4_col7\" class=\"data row4 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row4_col8\" class=\"data row4 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row5\" class=\"row_heading level0 row5\" >5_c</th>\n",
              "      <td id=\"T_7640e_row5_col0\" class=\"data row5 col0\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row5_col1\" class=\"data row5 col1\" >0.000234</td>\n",
              "      <td id=\"T_7640e_row5_col2\" class=\"data row5 col2\" >0.000064</td>\n",
              "      <td id=\"T_7640e_row5_col3\" class=\"data row5 col3\" >0.997756</td>\n",
              "      <td id=\"T_7640e_row5_col4\" class=\"data row5 col4\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row5_col5\" class=\"data row5 col5\" >0.001946</td>\n",
              "      <td id=\"T_7640e_row5_col6\" class=\"data row5 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row5_col7\" class=\"data row5 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row5_col8\" class=\"data row5 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row6\" class=\"row_heading level0 row6\" >6_b</th>\n",
              "      <td id=\"T_7640e_row6_col0\" class=\"data row6 col0\" >0.534869</td>\n",
              "      <td id=\"T_7640e_row6_col1\" class=\"data row6 col1\" >0.002683</td>\n",
              "      <td id=\"T_7640e_row6_col2\" class=\"data row6 col2\" >0.000191</td>\n",
              "      <td id=\"T_7640e_row6_col3\" class=\"data row6 col3\" >0.000001</td>\n",
              "      <td id=\"T_7640e_row6_col4\" class=\"data row6 col4\" >0.222202</td>\n",
              "      <td id=\"T_7640e_row6_col5\" class=\"data row6 col5\" >0.000193</td>\n",
              "      <td id=\"T_7640e_row6_col6\" class=\"data row6 col6\" >0.239860</td>\n",
              "      <td id=\"T_7640e_row6_col7\" class=\"data row6 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row6_col8\" class=\"data row6 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row7\" class=\"row_heading level0 row7\" >7_c</th>\n",
              "      <td id=\"T_7640e_row7_col0\" class=\"data row7 col0\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row7_col1\" class=\"data row7 col1\" >0.001894</td>\n",
              "      <td id=\"T_7640e_row7_col2\" class=\"data row7 col2\" >0.000003</td>\n",
              "      <td id=\"T_7640e_row7_col3\" class=\"data row7 col3\" >0.994302</td>\n",
              "      <td id=\"T_7640e_row7_col4\" class=\"data row7 col4\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row7_col5\" class=\"data row7 col5\" >0.003631</td>\n",
              "      <td id=\"T_7640e_row7_col6\" class=\"data row7 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row7_col7\" class=\"data row7 col7\" >0.000170</td>\n",
              "      <td id=\"T_7640e_row7_col8\" class=\"data row7 col8\" >0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_7640e_level0_row8\" class=\"row_heading level0 row8\" >8_b</th>\n",
              "      <td id=\"T_7640e_row8_col0\" class=\"data row8 col0\" >0.000001</td>\n",
              "      <td id=\"T_7640e_row8_col1\" class=\"data row8 col1\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row8_col2\" class=\"data row8 col2\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row8_col4\" class=\"data row8 col4\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row8_col6\" class=\"data row8 col6\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row8_col7\" class=\"data row8 col7\" >0.000000</td>\n",
              "      <td id=\"T_7640e_row8_col8\" class=\"data row8 col8\" >0.999999</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 238
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtUIgldHyhIh"
      },
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "\n",
        "## Transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40_cBX2yfCy"
      },
      "source": [
        "\n",
        "A **transformer** block is a collection of ideas. But, at its core, it contains a *self-attention layer* and *positional embeddings*.\n",
        "\n",
        "Other components include:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feedforward Layer\n",
        "\n"
      ],
      "metadata": {
        "id": "Nd8wUN7e-9t7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An additional 2-layer network (one hidden layer). This allows an additional transformation of the $A$ representation. Typically, the dimensionality is increased in this layer from $A$ to $O$. E.g.,\n",
        "\n",
        "$$O = FFN(A) = ReLU((AW_{o1})W_{o2})$$\n",
        "\n",
        "where $W_{o1}$ has dimension $d \\times 2d$, $W_{o2}$ has dimension $2d \\times d$ and $ReLU$ is activation function $=\\max(0, x)$. (2 is a bit arbitrary here.)"
      ],
      "metadata": {
        "id": "TLF4nvs0-lJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FFN example:\n",
        "W_o1 = torch.tensor([[.1,.2, .3,.4],[-.1,-.2, -.3, -.4]]) # 2x4\n",
        "W_o2 = torch.tensor([[.31,.22], [.23,.41],[-.11,-.22], [-.35, -.4]]) # 4x2\n",
        "O = nn.ReLU()((A @ W_o1) @ W_o2)\n",
        "O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ySNxGI-AeFO",
        "outputId": "1f2fa963-1be7-43ac-cd2f-21c079a2936b"
      },
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0761, 0.0967],\n",
              "        [0.0761, 0.0967],\n",
              "        [0.1426, 0.1813],\n",
              "        [0.0000, 0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 255
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head attention\n",
        "\n"
      ],
      "metadata": {
        "id": "YrbVwfb9-zWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple attention layers are run for each input sentence. This allows different layers to pay attention to different parts of the input. E.g., one can pay attention to syntax, another semantics, etc.\n",
        "\n",
        "E.g., for a 4-head attention block, run the entire attention network 4 times for each word, then **concatenate** all the $A$ outputs together. Pass to the FFN.\n",
        "\n",
        "![figs/m10multi.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10multi.png?raw=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "AcSkxy-7-6kk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Residual Connections"
      ],
      "metadata": {
        "id": "WG3A7mLJDL0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A simple trick that adds the input to the final output of a network.\n",
        "\n",
        "- For both forward and gradient computations, helps information get transferred more efficiently between layers.\n",
        "- Particularly helpful in deep networks (many layers) to that information is not forgotten.\n",
        "- Somewhat analogous to vanishing gradient problem\n",
        "\n",
        " $$X_i = X_{i-1} + \\mathrm{Layer}(X_i)$$\n",
        "\n",
        " E.g., for the simple attention network.\n",
        "\n",
        " output = A + sentence_embedding\n"
      ],
      "metadata": {
        "id": "_j2Fac9XDKIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Layer norm"
      ],
      "metadata": {
        "id": "PjB-FlzCDLZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute *z*-scores for values in each layer (subtract mean and divide by standard deviation)\n",
        "  - can improve training convergence by placing weights in same \"scale\" across layers\n",
        "  - Normalization is done separately for each word in the input:\n",
        "\n",
        "$$\\mu_i = \\frac{1}{d}\\sum_{j=1}^d \\mathbf{a}_{ij}$$\n",
        "\n",
        "$$\\sigma_i = \\sqrt{\\frac{1}{d}\\sum_{j=1}^d (\\mu_i - \\mathbf{a}_{ij})}$$\n",
        "\n",
        "$$\n",
        "\\mathbf{a}_i = \\frac{\\mathbf{a}_i - \\mu_i}{\\sigma}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "v7uBVzfCEiPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full transformer"
      ],
      "metadata": {
        "id": "E8ngAmDeFwK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For input X representing $N$ input words each with embedding size $d$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "T^1 = & \\hbox{SelfAttention}(X)\\\\\n",
        "T^2 = & X + T^1~~~~~~~~~\\hbox{residual connection}\\\\\n",
        "T^3 = & \\hbox{LayerNorm}(T^2)\\\\\n",
        "T^4 = & \\hbox{FFN}(T^3)\\\\\n",
        "T^5 = & T^4 + T^3~~~~~~~~\\hbox{residual connection}\\\\\n",
        "H = & \\hbox{LayerNorm}(T^5)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "![figs/m10transformer.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/m10transformer.png?raw=1)\n"
      ],
      "metadata": {
        "id": "8D90B7hgFrVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Transformer Variants"
      ],
      "metadata": {
        "id": "618Wxb97HKrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Original Transformer"
      ],
      "metadata": {
        "id": "DRQW6HcKHhBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The transformer was originally introduced for a machine translation task. Below is the figure from original paper \"Attention Is All You Need\" (2017)\n",
        "\n",
        "![figs/attention.png](https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/attention.png?raw=1)"
      ],
      "metadata": {
        "id": "WebrEynpHfnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT"
      ],
      "metadata": {
        "id": "NROGQ-JRHkGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is a \"multi-layer bidirectional Transformer encoder\"\n",
        "- to train, mask words at random so that information about word $i$ is not used when predicting word $i$\n",
        "\n",
        "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/bert.png?raw=1\" width=60%/>\n",
        "\n",
        "**Just the Encoder portion of the original Transformer architecture**\n"
      ],
      "metadata": {
        "id": "47xxiFUq-jIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT: Generative Pre-trained Transformer\n",
        "\n"
      ],
      "metadata": {
        "id": "DrChkcjqH1cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Just the Decoder portion of the original Transformer architecture**\n",
        "\n",
        "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/sequence/figs/gpt.png?raw=1\" width=60%/>\n"
      ],
      "metadata": {
        "id": "tbpRYpvyH4gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELMO\n"
      ],
      "metadata": {
        "id": "Vqchwi7xH6dR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Not a transformer!**\n",
        "\n",
        "ELMO \"word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.\"\n",
        "\n",
        "<img src=\"https://github.com/tulane-cmps6730/main/blob/main/lec/language_models/figs/elmo.png?raw=1\" width=60%/>\n",
        "\n"
      ],
      "metadata": {
        "id": "7P1pY0EmH7-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Group Task"
      ],
      "metadata": {
        "id": "LHbatGTV-fVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Re-run the `SimpleSelfAttention` example above for the long-distance example (\"bcabcbcbca\"), but this time do not use the positional embeddings. Is it able to capture the dependencies as well. Why or why not? Inspect the attention weights and other components of the model to see if you can identify what is different."
      ],
      "metadata": {
        "id": "DA7nShN2JLnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr size=10 color=#285C4D>\n",
        "\n",
        "## Sources"
      ],
      "metadata": {
        "id": "MQ31VULeI8s9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1YwQFKzvHdB"
      },
      "source": [
        "\n",
        "\n",
        "- https://web.stanford.edu/class/cs224n/\n",
        "\n",
        "- Andrej Karpathy's excellent, simplified implementation of transformer: https://github.com/karpathy/minGPT\n",
        "  - Along with code walkthrough video: https://www.youtube.com/watch?v=kCc8FmEb1nY"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "f8OiXeEVwfGw",
        "ioD31_UvxUbm",
        "f5fR8nbqFtoZ",
        "gw4vuJPBFyib",
        "q0SFcge4F0pa",
        "zrxdpPuCIT_f",
        "EcA6NlwQCqol",
        "kjEk0qGS175l",
        "XtUIgldHyhIh",
        "YrbVwfb9-zWA",
        "WG3A7mLJDL0S",
        "PjB-FlzCDLZW",
        "E8ngAmDeFwK7",
        "618Wxb97HKrU",
        "DRQW6HcKHhBm",
        "NROGQ-JRHkGs",
        "DrChkcjqH1cv",
        "LHbatGTV-fVw",
        "MQ31VULeI8s9"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}